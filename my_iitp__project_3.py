# -*- coding: utf-8 -*-
"""My_IITP__Project 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VFtuK5QjykxCm7PGSSPRwQ9DfulHUU1S

### Working Environment

#**LLM-Driven Sentiment Cognition**

##**Kumar Pancham Prasar**
###***M.tech, IIT PATNA***
###***Roll Number: 2303res23***
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/sample_data
!ls

"""### Import Dataset"""

import pandas as pd

data = pd.read_csv('/content/sample_data/data/amazon_alexa.tsv', sep='\t')
data.head(10)

mydata = data[['verified_reviews','feedback']]
mydata.columns = ['review','label']

mydata.head()

mydata.value_counts('label')

# Count the occurrences of each label
label_counts = mydata["label"].value_counts()

# Get the number of rows to drop from the majority class
rows_to_drop = label_counts.max() - label_counts.min()

# Drop rows from the majority class randomly
if rows_to_drop > 0:
   data_majority = mydata[mydata["label"] == 1]
   data_balanced = mydata.drop(data_majority.sample(rows_to_drop).index)
else:
   data_balanced = mydata.copy()

# Check the new class balance
print(data_balanced["label"].value_counts())

"""## Data Preprocessing"""

import re

def clean_text(text):
  # Remove special characters and punctuation
  text = re.sub(r"[^\w\s]", " ", text)

  # Remove single characters
  text = re.sub(r"\b[a-zA-Z]\b", " ", text)

  # Remove HTML tags
  text = re.sub(r"<[^>]*>", " ", text)

  # Lowercase the text
  text = text.lower()

  # Remove extra whitespace
  text = re.sub(r"\s+", " ", text)

  # Trim leading and trailing spaces
  text = text.strip()

  return text

import pandas as pd

# Extract the review column as a list and convert to string
reviews = data_balanced['review'].astype(str).tolist()

# Clean the text in the list
cleaned_reviews = [clean_text(review) for review in reviews]

# Add the cleaned reviews as a new column to the DataFrame
data_balanced['clean_reviews'] = cleaned_reviews

data_balanced

"""## Data Split

95% test set sample and 5% training set sample for few shot promting
"""

import pandas as pd

# Assuming your DataFrame is called "df"
total_rows = len(data_balanced)
test_size = int(total_rows * 0.95)

# Randomly sample train_size rows for the training set
test_set = data_balanced.sample(test_size)

# Get the remaining rows for the test set
train_set = data_balanced.drop(test_set.index)

"""## Sentiment w/ LLM

### Setting up Gemini API
"""

!pip install -q -U google-generativeai

# Necessary packages
import pathlib
import textwrap

import google.generativeai as genai

from IPython.display import display
from IPython.display import Markdown


def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)

for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

model = genai.GenerativeModel('models/gemini-1.5-pro-latest')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = genai.GenerativeModel('models/gemini-1.5-pro-latest')
# response = model.generate_content("What is the meaning of life?")
# 
# to_markdown(response.text)

"""#### Single API Call"""

test_set_sample = test_set.sample(20)

test_set_sample['pred_label'] = ''

test_set_sample

# Convert the DataFrame to JSON using the to_json() method

json_data = test_set_sample[['clean_reviews','pred_label']].to_json(orient='records')

# Print the JSON data
print(json_data)

prompt = f"""
You are an expert linguist, who is good at classifying customer review sentiments into Positive/Negative labels.
Help me classify customer reviews into: Positive(label=1), and Negative(label=0).
Customer reviews are provided between three back ticks.
In your output, only return the Json code back as output - which is provided between three backticks.
Your task is to update predicted labels under 'pred_label' in the Json code.
Don't make any changes to Json code format, please.

```
{json_data}
```
"""

print(prompt)

response = model.generate_content(prompt)

print(response.text)

import json
import re

# Use a regular expression to find the JSON array within the response text
json_match = re.search(r'\[.*\]', response.text, re.DOTALL)

if json_match:
    json_data = json_match.group(0)
    # Load the cleaned data and convert to DataFrame
    data = json.loads(json_data)
    df_sample = pd.DataFrame(data)

    display(df_sample)
else:
    print("Could not find a valid JSON array in the response.")

# prompt: Overwrite pred_label from 'df' into pred_label in 'train_set_sample'

test_set_sample['pred_label'] = df_sample['pred_label'].values
test_set_sample

# Plotting confusion matrix on the predictions

from sklearn.metrics import confusion_matrix

y_true = test_set_sample["label"]
y_pred = test_set_sample["pred_label"]

confusion_matrix(y_true, y_pred)

"""#### Batching API Calls (Single Shot)"""

test_set.shape

test_set_total = test_set.sample(100)

test_set_total['pred_label'] = ''

test_set_total

batches = []
batch_size = 50

for i in range(0, len(test_set_total), batch_size):
  batches.append(test_set_total[i : i + batch_size])  # Append batches instead of assigning

import json
import re
import pandas as pd # Ensure pandas is imported

df_total = pd.DataFrame()  # Initialize an empty DataFrame

# Ensure the 'responses' variable is populated by running the previous cell (cell with id 6kT-noQv_3Un)
for response in responses:
  # Check if the response text is not empty and contains potential JSON
  if response and response.text:
    # Use a regular expression to find the JSON array within the response text
    json_match = re.search(r'\[.*\]', response.text, re.DOTALL)

    if json_match:
        json_data = json_match.group(0)
        try:
            # Load the cleaned data and convert to DataFrame
            data = json.loads(json_data)
            df_temp = pd.DataFrame(data)

            # Append the DataFrame to the final DataFrame
            df_total = pd.concat([df_total, df_temp], ignore_index=True) # Use pd.concat instead of append
        except json.JSONDecodeError:
            print(f"Could not decode JSON from response: {response.text}")
            # Optionally, append a DataFrame with default values or handle this case as needed
            # For now, we'll skip this response
    else:
        print(f"Could not find a valid JSON array in the response: {response.text}")
  else:
    print("Received an empty response or response text.")


display(df_total)  # Display the final DataFrame

"""import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score

# Extract true and predicted labels
y_true = test_set_total["label"]
y_pred = test_set_total["pred_label"]

# Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)
accuracy = accuracy_score(y_true, y_pred)

# Print confusion matrix and accuracy
print("Confusion Matrix:\n", cm)
print(f"\nAccuracy: {accuracy:.2f}")

# Plot confusion matrix
plt.figure(figsize=(6, 5))
plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.colorbar()
tick_marks = np.arange(len(set(y_true)))
plt.xticks(tick_marks, set(y_true), rotation=45)
plt.yticks(tick_marks, set(y_true))

# Adding labels
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.grid(False)

# Annotate cells
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, str(cm[i, j]), ha='center', va='center', color='black')

plt.show()

### Batching API Calls: Gemini API
"""

test_set.shape

test_set_total = test_set.sample(100)

test_set_total['pred_label'] = ''

test_set_total

batches = []
batch_size = 25

for i in range(0, len(test_set_total), batch_size):
  batches.append(test_set_total[i : i + batch_size])  # Append batches instead of assigning

import time

def gemini_completion_function(batch,current_batch,total_batch):
  """Function works in three steps:
  # Step-1: Convert the DataFrame to JSON using the to_json() method.
  # Step-2: Preparing the Gemini Prompt
  # Step-3: Calling Gemini API
  """

  print(f"Now processing batch#: {current_batch+1} of {total_batch}")

  json_data = batch[['clean_reviews','pred_label']].to_json(orient='records')

  prompt = f"""You are an expert linguist, who is good at classifying customer review sentiments into Positive/Negative labels.
  Help me classify customer reviews into: Positive(label=1), and Negative(label=0).
  Customer reviews are provided between three backticks below.
  In your output, only return the Json code back as output - which is provided between three backticks.
  Your task is to update predicted labels under 'pred_label' in the Json code.
  Don't make any changes to Json code format, please.
  Error handling instruction: In case a Customer Review violates API policy, please assign it default sentiment as Negative (label=0).

  ```
  {json_data}
  ```
  """

  print(prompt)
  response = model.generate_content(prompt)
  time.sleep(5)

  return response

batch_count = len(batches)
responses = []

for i in range(0,len(batches)):
  responses.append(gemini_completion_function(batches[i],i,batch_count))

import json
import re
import pandas as pd # Ensure pandas is imported

df_total = pd.DataFrame()  # Initialize an empty DataFrame

for response in responses:
  # Check if the response text is not empty and contains potential JSON
  if response and response.text:
    # Use a regular expression to find the JSON array within the response text
    json_match = re.search(r'\[.*\]', response.text, re.DOTALL)

    if json_match:
        json_data = json_match.group(0)
        try:
            # Load the cleaned data and convert to DataFrame
            data = json.loads(json_data)
            df_temp = pd.DataFrame(data)

            # Append the DataFrame to the final DataFrame
            df_total = pd.concat([df_total, df_temp], ignore_index=True) # Use pd.concat instead of append
        except json.JSONDecodeError:
            print(f"Could not decode JSON from response: {response.text}")
            # Optionally, append a DataFrame with default values or handle this case as needed
            # For now, we'll skip this response
    else:
        print(f"Could not find a valid JSON array in the response: {response.text}")
  else:
    print("Received an empty response or response text.")


display(df_total)  # Display the final DataFrame

# prompt: Overwrite pred_label from 'df' into pred_label in 'train_set_sample'

test_set_total['pred_label'] = df_total['pred_label'].values
test_set_total

# Plotting confusion matrix on the predictions

from sklearn.metrics import confusion_matrix

y_true = test_set_total["label"]
y_pred = test_set_total["pred_label"]

confusion_matrix(y_true, y_pred)

# Plotting confusion matrix on the predictions
# Confusion matrix layout for binary classification:
#                Predicted
#                0     1
# Actual  0     TN    FP
#         1     FN    TP

# TN (True Negative): correctly predicted 0 (e.g., predicted negative, and it was negative)
# FP (False Positive): predicted 1, but it was actually 0 (false alarm)
# FN (False Negative): predicted 0, but it was actually 1 (missed positive)
# TP (True Positive): correctly predicted 1 (true positive detection)

# Goal: maximize TP and TN, minimize FP and FN


from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

y_true = test_set_total["label"]
y_pred = test_set_total["pred_label"]

# Confusion Matrix
print("Confusion Matrix:")
print(confusion_matrix(y_true, y_pred))

# Accuracy
accuracy = accuracy_score(y_true, y_pred) * 100
print(f"\nAccuracy: {accuracy:.2f}%")

# Classification Report (in %) excluding 'support'
report = classification_report(y_true, y_pred, output_dict=True)

print("\nClassification Report (in %):")
for label, metrics in report.items():
    if isinstance(metrics, dict):
        print(f"\nLabel {label}:")
        for metric, value in metrics.items():
            if metric != "support":
                print(f"  {metric}: {value * 100:.2f}%")
    else:
        print(f"\n{label}: {metrics * 100:.2f}%")